# 2023-01-14
## `MPI_Scatterv`, `MPI_Gattherv` and `MPI_Allgatherv`
The common thread between these three (and perhaps more) collective
communication functions are the two buffers `sendcounts` and `displs` (see `man 3 MPI_Gatherv` or [view online](https://www.open-mpi.org/doc/v3.1/man3/MPI_Gatherv.3.php)).

I found myself writing code to construct these two buffers every
time for roughly equal distribution. Here is the convenience function.

```C
typedef struct
{
    int *sendcounts;
    int *displs;
} vparams_t;

int vparams_create(vparams_t *vp, int count, int nprocs)
{
    int *sendcounts = malloc(sizeof(int) * nprocs);
    check_mem(sendcounts);
    int *displs = malloc(sizeof(int) * nprocs);
    check_mem(displs);
    int quot = count / nprocs;
    int rem = count % nprocs;
    int running_displ = 0;
    for (int i = 0; i < nprocs; i++)
    {
        int sc = quot;
        if (i < rem)
            sc++;
        sendcounts[i] = sc;
        displs[i] = running_displ;
        running_displ += sc;
    }

    vp->sendcounts = sendcounts;
    vp->displs = displs;
    return 0;
error:
    return 1;
}
```

## Presentation on Error Handling in C
Came across these nice [slides](https://resources.sei.cmu.edu/asset_files/Presentation/2016_017_101_484207.pdf).

# 2023-01-15
## MPI - Basics of Creating Communicators and Process Topologies 
As it turns out, you do not always have to fiddle around with index calculations to calculate the ranks of
your neighbouring process(es). `MPI_Cart_create` and friends can handle some of that for you.

Here is a minimum working example (MWE) for creating a 1D torus.
```C
#include <stdio.h>
#include <stdlib.h>
#include <mpi.h>
#include <stdbool.h>

int main(int argc, char *argv[])
{
    int rank, nprocs;
    MPI_Comm commring;
    MPI_Init(&argc, &argv);
    MPI_Comm_size(MPI_COMM_WORLD, &nprocs);
    MPI_Comm_rank(MPI_COMM_WORLD, &rank);

    int const period = 1, ndims = nprocs;
    bool reorder = true;
    MPI_Cart_create(MPI_COMM_WORLD, 1, &ndims, &period, reorder, &commring);
    int left, right;
    MPI_Cart_shift(commring, 0, 1, &left, &right);

    fprintf(stderr, "rank %d: left neighbour: %d, right neighbour: %d\n", rank, left, right);
    MPI_Finalize();
    return 0;
}
```
Please don't enable too many warning flags because we're ignoring a lot of return values. Higher dimensions are possible, but am yet
to try it. Consider consulting the [man pages](https://www.open-mpi.org/doc/v4.1/man3/MPI_Cart_create.3.php) `man 3 MPI_Cart_create`.

This month I am doing a course that has an MPI assignment. Hence the entries on MPI.

## Domain Decomposition Resources
A relatively compact summary of Galerkin approximations, Sobolev spaces, and other preconditioning
methods is given in the appendices of Toselli and Widlund's book. I should probably consolidate all info related to my thesis
in a separate place

# 2023-01-16
## On Focus and Crises
I started reading "Life on a Knife's Edge" by Dr Rahul Jandial. In chapter two, "Performance", Dr Jandial
shares some information regarding staying calm and focused in stressful situations. The original text is
an excellent account and what follows is merely a wattered down summary for my own reference later.

**Alarm Overload**: More alarms are perceived than can be addressed.

E.g. during the partial core meltdown at the Three Mile Island nuclear power plant, operators were disoriented by
all the alarms and didn't know which alarms were the most pressing and which could be ignored.

To respond quickly and effectively in a crisis, we need a way to bring order to the external alarms, to _find focus by ignoring
emotional distractors_. When you are being inundated with stimuli, a cluster of neurons known as the basal glandia actively filter
out what is deemed unimportant.

Note the exact phrasing here -- it does not emphasise the important stimulus, but filters out the unimportant.
Neural activity is lower in experts because they don't dial up the focus, but dial down the distraction and stress.
The ability to be vigilant is built-in, but the ability to ignore must be cultivated.

### Breathing Patterns, Brain Activity and Staying in Control
Controlling your breathing is the most powerful weapon you have to strengthen your emotional regulation and improve your performance.
Don't just use it as safety valve in times of crisis, but train it regularly. Dr Jandial noticed that other experienced surgeons used
tape to seal their surgical masks to the bridge of their nose in prevent fogging the magnification lenses worn by them. Under pressure,
your breathing becomes fast and heavy. Dr Jandial does not use tape because when his FOV fogs up, he knows that he is doing something wrong
and needs to control his breath.

The adaptive state that we tend to slip into during stressful situations is one of hyperventilation; the brain readies the lungs and
diaphragm. Meditative breathing can moderate the electrical activity in the brain; your breathing patterns can steer your brain towards
both breathing and calm.

"I used the time to steady my breathing. In situations like this, I don't breathe more deeply, I breathe more slowly.
Three seconds in; three seconds out. No sudden inhalation with mouth agape. I find it easier to pace my breathing by
inhaling and exhaling through my nose."

## (TODO) Customizing Broadcast in Julia
High level overview. Broadcasting something that subtypes `AbstractArray` and implements its interface properly is somewhat
easy. Other types can be non-trivial. This is my own understanding and could be incorrect. Once I have a working broadcast
for a non-array type, I will put it in another daybook entry.

Relevant types and methods.
- `Broadcast.BroadcastStyle`: An empty struct specialized for your type. Dispatch on relevant binary methods to combine styles into destination styles
- `Broadcast.Broadcasted`: A tree-like container for lazily evaluating a compound broadcasted expression 
- `Broadcast.instantiate`: Takes a `Broadcast.Broadcasted` and figues out the final axes type and value
- `Broadcast.materialize`: Performs the broadcast operation
- `Base.copyto!(dest, ::Broadcast.Broadcasted)`: Method called by `materialize` to perform calculations and store results in destination style

# 2023-01-17
## Iterative Solvers
I should probably know the following Krylov subspace solvers like the back of my hand, and probably even be
able to hammer out an implementations in Julia for using the `AbstractVectors` and `AbstractMatrix`
interfaces/abstract types/concepts/whatever on a whim. Every time I need anything related to iterative solvers,
I find myself studying them again. What is this? Ultra-mega-early onset of dementia? Do I have to make Anki cards
for them?
- Conjugate Gradient
- Generalized Minimal RESidual (GMRES)
Probably some more as well. See chapter 3 of "An Introduction to Domain Decomposition Methods" by Dolean, Jolivet and Nataf.
See 

## Laying Pipes
Say you're solving something like an advent of code problem, or parsing the contents of a log file. You have your own awesome
struct and overloaded `Base.parse` for it. Neat. If my input is a file delimited by padded spaces, then my first instinct is to
write something like.

```julia
function read_file(filename)
    file_handle = open(filename, "r")
    raw_input = read(file_handle)
    lines = split(raw_input, '\n')
    split_lines = split.(lines, r"\s+")
    [parse.(MyStruct, sln) for sln in split_lines]
end
```
Okay. We do a little type-piracy.
```julia
import Base:Fix1, Fix2, parse, split

# return partial applications of the functions with the second and first arguments fixed respectively
Base.split(dlm) = Fix2(split, dlm)
Base.parse(T) = Fix1(parse, type)
```

And it turns out that `open` has a unary method as well. Neat.
Suppose the contents of your file look like this.
```
315.565     10.997      2.644      5.488      5.687      5.160
   874.257    767.358      4.232    -11.330     -2.686      0.586
   512.743    213.092    -15.247    -13.149     -0.448     -5.424
   235.152     45.033      9.275     12.296      2.989     -1.023
   842.798    744.148     32.135    -38.384      2.812     -7.190
   502.337    247.592    -96.554    -84.124     -1.308     -4.590
   921.245    185.916    -33.037    -29.464     -1.866     -4.484
   865.638    280.926    -69.513     -3.999      0.871      1.829
    33.166    741.856    179.162   -128.860      0.932      3.294
   738.681    284.072   -416.399    -11.975      0.574      0.121
   499.598    239.011  -1714.422  -1039.006     -2.711     -7.107
   797.560    591.609   -542.754    669.172     -5.231      3.688
    58.047    729.860    940.613   -803.977      2.511     -4.639
   724.403    590.149   1988.153    900.726     -0.618      4.578
   517.308    490.942  -2649.969   3266.692     -5.906      0.523
   769.050     20.127   -217.198   2251.789     -3.926      4.559
  1015.302    470.902  -4943.484   3031.763      2.137     -3.906
   757.257    599.449  -2255.915    493.940      0.358      0.925
   584.198    310.795 -16036.162 -10295.909     -2.380      3.089
   642.416    285.613 -32956.792  -1169.868     -2.354      1.182
   504.942    325.752 -23706.980 -32634.931     -0.754      2.626
   144.998    359.160 -70994.391 -84262.946      0.588     -3.398
   340.664    313.958  54243.778 -104072.175      4.175      0.304
   587.066      9.683 -43411.226  87127.767     -0.388      6.419
   226.697    556.440  61018.432 -35868.765      5.967     -5.459
   390.242    722.851 166465.707 -213186.875      1.707     -3.869
   518.610    136.944 -228552.543 144358.764     -6.112      4.731
   930.590     12.795 -67351.646  40268.283     -3.140     -0.220
    81.907    310.175 271991.706  96072.287      0.852      2.720
   924.353    680.982 -103482.403 -57526.304     -3.617      1.785
   481.186    638.043 -187590.505  -1379.635     -2.104      0.079
   389.939    196.293 230161.304 164102.709      0.356      0.934
```

Now you can just
```julia
file_name |> open |> read |> String |> strip |> split('\n') .|> strip .|> split(r"(\s+)")
```
to get
```
 ["315.565", "10.997", "2.644", "5.488", "5.687", "5.160"]
 ["874.257", "767.358", "4.232", "-11.330", "-2.686", "0.586"]
 ["512.743", "213.092", "-15.247", "-13.149", "-0.448", "-5.424"]
 ["235.152", "45.033", "9.275", "12.296", "2.989", "-1.023"]
 ["842.798", "744.148", "32.135", "-38.384", "2.812", "-7.190"]
 ["502.337", "247.592", "-96.554", "-84.124", "-1.308", "-4.590"]
 ["921.245", "185.916", "-33.037", "-29.464", "-1.866", "-4.484"]
 ["865.638", "280.926", "-69.513", "-3.999", "0.871", "1.829"]
 ["33.166", "741.856", "179.162", "-128.860", "0.932", "3.294"]
 ["738.681", "284.072", "-416.399", "-11.975", "0.574", "0.121"]
 ["499.598", "239.011", "-1714.422", "-1039.006", "-2.711", "-7.107"]
 ["797.560", "591.609", "-542.754", "669.172", "-5.231", "3.688"]
 ["58.047", "729.860", "940.613", "-803.977", "2.511", "-4.639"]
 ["724.403", "590.149", "1988.153", "900.726", "-0.618", "4.578"]
 ["517.308", "490.942", "-2649.969", "3266.692", "-5.906", "0.523"]
 ["769.050", "20.127", "-217.198", "2251.789", "-3.926", "4.559"]
 ["1015.302", "470.902", "-4943.484", "3031.763", "2.137", "-3.906"]
 ["757.257", "599.449", "-2255.915", "493.940", "0.358", "0.925"]
 ["584.198", "310.795", "-16036.162", "-10295.909", "-2.380", "3.089"]
 ["642.416", "285.613", "-32956.792", "-1169.868", "-2.354", "1.182"]
 ["504.942", "325.752", "-23706.980", "-32634.931", "-0.754", "2.626"]
 ["144.998", "359.160", "-70994.391", "-84262.946", "0.588", "-3.398"]
 ["340.664", "313.958", "54243.778", "-104072.175", "4.175", "0.304"]
 ["587.066", "9.683", "-43411.226", "87127.767", "-0.388", "6.419"]
 ["226.697", "556.440", "61018.432", "-35868.765", "5.967", "-5.459"]
 ["390.242", "722.851", "166465.707", "-213186.875", "1.707", "-3.869"]
 ["518.610", "136.944", "-228552.543", "144358.764", "-6.112", "4.731"]
 ["930.590", "12.795", "-67351.646", "40268.283", "-3.140", "-0.220"]
 ["81.907", "310.175", "271991.706", "96072.287", "0.852", "2.720"]
 ["924.353", "680.982", "-103482.403", "-57526.304", "-3.617", "1.785"]
 ["481.186", "638.043", "-187590.505", "-1379.635", "-2.104", "0.079"]
 ["389.939", "196.293", "230161.304", "164102.709", "0.356", "0.934"]
```
Disgusting. Just use `DataFrames.jl`.

# 2023-01-19
## Your Research and You
The transcript of a talk by Richard Hamming at Bell Labs.

https://www.cs.virginia.edu/~robins/YouAndYourResearch.html

I will write a summary once I finish reading it.

**Update**
After having worked at Los Alamos and eventually joining Bell labs,
Hamming wanted to know what made the difference between those who
do great research and those who don't. Great here means something
"Nobel-prize worthy" even if it doesn't actually win the prize itself.
He had been colleagues with Shannon, Openheimer, Hans Bethe, Fermi
et al.

"Luck favors the prepared mind." -- Pasteur. There is a whole section
dedicated to luck and people's perception of its importance in doing
great science. He stated something intriguing about Einstein as an
example of great scientists having courageous, independent thoughts.
At the age of 12-14 Einstein had thought about what light would look
like if the observer was traveling at the speed of light. From
a Galilean frame he would see a stationary local extrema in vacuum,
which is unphysical. Was it luck that he finally created STR?

Moving on from luck to brains. Hamming said that doing great
work requires something more than "mere brains". By brains he meant
abstract reasoning ability. He gave the example of Bill Pfann, how his eventual success begain with him not knowing much mathematics and being inarticulate. But, once taught to run computers, he eventually
won all the prizes in his field. He became much more productive with increasing achievement.

Courage. The courage to believe that you can do important problems. Shannon's example...
## Optimization Techniques for GPU Programming
New paper dropped -- [https://dl.acm.org/doi/pdf/10.1145/3570638](https://dl.acm.org/doi/pdf/10.1145/3570638). 74 pages in total,
26 pages of main content, remaining is appendix and references. This is a survey of 450(!) papers. It covers optimizations that GPU
programmers can apply when using whatever programming language/model they use. Doing a first pass of the paper, I have come to realize
that I should be doing a lot more GPU programming in my life. How does a guy incorporate more GPU programming in his work, hobby projects
etc? How does a guy incorporate hobby projects in the first place while maintaining a good work-life balance? Perhaps I am not cut out
for programming and should start working in Haldiram's or something.

Well, anyway. I am incredibly grateful that this paper got accepted when it did.

Some reminders about GPU architecture and programming models from the paper -- most of these should be familiar to anyone who has programmed
(NVIDIA) GPUs for compute (and not necessarily graphics, I know nothing about graphics) before.

- Streaming Multiprocessors (SMs) are organized into many cores and further consist of units like
  - Floating Point Units (FPUs)
  - Special Function Units (SFUs) for transcendental functions
  - Load/store (LS) units
- Threads are organized in a hierarchy
  - 32 threads form a warp -- warps are the unit of scheduling and execute in lockstep
  - Threads are grouped into blocks
  - Blocks are grouped into a grid
- Each block is "mapped" (scheduled?) to an SM (further reading -- [How the Fermi Thread Block Scheduler Works](https://www.cs.rochester.edu/~sree/fermi-tbs/fermi-tbs.html))
- The compiler allocates registers on a per-thread basis depending on the kernel code. _Register spills go to device memory_
- Long latency operations (e.g. load from device memory) in a warp can be hidden in two ways
  - Thread-level parallelism (TLP) -- scheduling a ready warp
  - Instruction Level Parallelism (ILP) -- scheduling an independent instruction stream in the same warp
  I had no idea that GPU cores were ILP-capable. The paper mentions that even GPU vendors tend to leave this out.

An aside starting  from the last point about ILP -- the literature that they mention to corroborate the ILP point is none other than Vasily
Volkov's GTC 2010 presentation. His PhD dissertation was "Understanding latency hiding on GPUs", and I recall reading one of his papers titled 
something along the lines of "Tuning Dense Linear Algebra on GPUs", which showed better dense linear algebra kernel performance at lower
occupancies. I remember getting sidetracked towards Saavedra et al's pointer chase benchmarking paper around that time. I should consider
picking Volkov's paper again.

Will add more notes on specific optimizations when I read further.

# 2023-01-20
## SICM and Functional Differential Geometry
These are two books co-authored by Gerald J. Sussman. He has made some
[awesome utilities](http://groups.csail.mit.edu/mac/users/gjs/6946/installation.html) for (MIT) Scheme for the SICM book/course. Having
been taught classical mechanics from Goldstein (I hated it), I have a feeling that SICM gets close to the best way of teaching classical
mechanics and its mathematical formalisms.
I would love to work through both books some day. Right now my naive/dumb self was hoping to get some functional analysis basics  out
of "Functional Differential Geometry", but turns out they're quite different. 

It would be nice to have scmutils in some form in Julia. Using Symbolics.jl? Or macros? A bit of both? I do not know which approach will be "good".
Maybe I will try experimenting with them sometime soon $^\text{TM}$.

# 2023-01-21
## A Quick Reference for Numerical Analysis
If you are working in Numerical Analysis or related fields, you probably want this in your back pocket.

https://github.com/higham/what-is

You probably already knew about this.

## Notes from GPU Optimization Techniques
A few days back I mentioned [this paper](https://dl.acm.org/doi/pdf/10.1145/3570638).

### Balancing Parameters for Performance
It's evident that there are a lot of kernel parameters to balance. The popular guideline is (if not was) to maximize occupancy, and the
CUDA SDK comes with a whole occupancy-calculator spreadsheet. But, Volkov states that occupancy is only a measure of TLP utilization and
completely ignores ILP -- it is possible to achieve more FLOPS and lower occupancy. Low occupancy is even required for some kernels' perf.

If the number of registers per block is too high, then fewer blocks are active simultaneously as not enough hardware registers might be
available to service all the blocks.

The authors present a few categories as guidelines for presenting the optimizations themselves. The exact membership
of an optimizing transformation can be fuzzy sometimes.
- Memory Access
  - On-Chip
  - Off-Chip
- Irregularity -- this is probably going to be important for my thesis
- Balancing
  - Instruction Stream
  - Parallelism
  - Synchronization
- Host-interaction

### Memory Access
Texture memory is optimized for spatial locality in 2D. I wonder if recent SDKs have data structures, if not the architecture having
dedicated hardware, for optimized spatial locality in higher dimensions. I recall seeing something like surface memory 3D in the doxygen'd man pages
somewhere, but anyway I digress.

**Texture memory** can be used for handling boundary values (sometimes called ghost or halo cells?), which makes sense since they're
optimized for 2D sptial locality.
In general texture memory can be used to improve uncoalesced access. Consider browsing the mentioned references.

An interesting detail about shared memory that I seem to have missed -- **synchronization at shared mem access occurs using barriers** at the
block-level, which naturally _stalls warps_. Potential optimization: decrease the shared mem block size,  allow multiple thread blocks to run
on a single SM.

For bank conflicts -- padding is the standard optimisation technique. Bernstein et al wrote a search tool that assigns threads with data
such that almost all bank conflicts are eliminated. Other techniques include reordering data, remapping threads to data, exploiting commutativity.

Man, these are so many optimizations and so many references I'll have to talk to Ben to be able to pick and choose wisely.

Use warp functions (voting, shuffle, maybe newer instructions in newer arch versions?) for thread-cooperative operations like reductions and
prefix-sum  without going through shared memory.

**TODO:** Check if cuda-samples covers this usage of shuffle functions

**TODO:** I did not quite understand this next subsection on register blocking and need to look further into references.

**Register blocking** -- registers are best for temporal locality, but not spatial locality as inter-thread comms via only registers is
restricted to warp-shuffle functions. _Register blocking is highly related to _loop unrolling_ and _varying the work per thread_, because
the same data is reused for multiple elements that a thread computes.

There's even template metaprogramming (TMP) techniques for controlling arrays of registers. Do I want to learn TMP? No. Will I have to? No. Would it be fun replicating it in Julia macros? Possibly. Do I have time for it? Probably not. 

Aside: the "heteroschedastic" presentation in performance engineering had mentioned this exact optimisation if I recall correctly -- varying
the work per thread, something along the lines of that a warp just wants work to do all the time Rush B no ~stall~ stop. 

Anyway, **TODO:** I should look into the references for register blocking and usages of texture memory as they might be important for
domain decomposition kernels.
